{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import numpy as np\n",
    "# from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(643499, 7)\n",
      "236781\n",
      "236781.0\n",
      "(227353, 9)\n",
      "454706\n",
      "454706\n",
      "(161284, 7)\n",
      "59483\n",
      "59483.0\n",
      "(57063, 9)\n",
      "114126\n",
      "114126\n",
      "(89198, 7)\n",
      "32774\n",
      "32774.0\n",
      "(31539, 9)\n",
      "63078\n",
      "63078\n"
     ]
    }
   ],
   "source": [
    "### FixedROUGE, sentence classification, classify paired source or target, length 10-150\n",
    "save_path = './src_100-1000_tgt_0-700_full_data_extract_elife_annals_medicine_reproductive_ordered_pairs_FixedROUGE/source_target_classification_10_150/'\n",
    "for data_type in ['train', 'val', 'test']:\n",
    "    data = pd.read_csv('./src_100-1000_tgt_0-700_full_data_extract_elife_annals_medicine_reproductive_ordered_pairs_FixedROUGE/'+data_type+'.csv')\n",
    "    print(data.shape)\n",
    "    data = data.dropna(subset=['src_sent'])\n",
    "    data = data.dropna(subset=['tgt_sent'])\n",
    "    print(len(data))\n",
    "    print(data['paired'].sum())\n",
    "\n",
    "    sentences = list(data['src_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['src_length'] = length_list\n",
    "    data = data[(data['src_length']>=10) & (data['src_length']<=150)]\n",
    "\n",
    "    sentences = list(data['tgt_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['tgt_length'] = length_list\n",
    "    data = data[(data['tgt_length']>=10) & (data['tgt_length']<=150)]\n",
    "\n",
    "    sentences = list(data['src_sent'])+list(data['tgt_sent'])\n",
    "    labels = list([0 for i in range(len(data['src_sent']))]) + list([1 for i in range(len(data['tgt_sent']))])\n",
    "    \n",
    "    # marker = {'phrase': ['Significance ', 'Background ', 'Objectives ', 'Main results ', \"Authors' conclusions \"], 'length': [13, 11, 11, 13, 21]}\n",
    "    # count = 0\n",
    "    # for i in range(len(sentences)):\n",
    "    #     for j in range(len(marker['phrase'])):\n",
    "    #         if sentences[i][:marker['length'][j]] == marker['phrase'][j]:\n",
    "    #             sentences[i] = sentences[i][marker['length'][j]:]\n",
    "    #             count += 1\n",
    "    # print(count)\n",
    "\n",
    "    idx = [i for i in range(len(sentences))]\n",
    "    random.shuffle(idx)\n",
    "    sentences = list(np.array(sentences)[idx])\n",
    "    labels = list(np.array(labels)[idx])\n",
    "\n",
    "    print(data.shape)\n",
    "    print(len(sentences))\n",
    "    print(len(labels))\n",
    "    with open (save_path + data_type + '.input0', 'w') as f:\n",
    "        for item in sentences:\n",
    "            f.write(item + '\\n')\n",
    "    with open (save_path + data_type + '.label', 'w') as f:\n",
    "        for item in labels:\n",
    "            f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(643499, 7)\n",
      "(168241, 9)\n",
      "(161284, 7)\n",
      "(42259, 9)\n",
      "(89198, 7)\n",
      "(23416, 9)\n"
     ]
    }
   ],
   "source": [
    "### FixedROUGE, sentence summarization, input paired source, output paired target \n",
    "### train set source: 10.0 - 200.0\n",
    "### train set target: 10.0 - 150.0\n",
    "### train set ROUGE: 0.1-0.4\n",
    "save_path = './ordered_pairs_src_200_tgt_150_rouge_0.1-0.4_FixedROUGE_pairs/'\n",
    "for data_type in ['train', 'val', 'test']:\n",
    "    data = pd.read_csv('./src_100-1000_tgt_0-700_full_data_extract_elife_annals_medicine_reproductive_ordered_pairs_FixedROUGE/'+data_type+'.csv')\n",
    "    print(data.shape)\n",
    "    data = data.dropna(subset=['src_sent'])\n",
    "    data = data.dropna(subset=['tgt_sent'])\n",
    "\n",
    "    sentences = list(data['src_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['src_length'] = length_list\n",
    "\n",
    "    sentences = list(data['tgt_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['tgt_length'] = length_list\n",
    "\n",
    "    data = data[(data['src_length']>=10) & (data['src_length']<=200)]\n",
    "    data = data[(data['tgt_length']>=10) & (data['tgt_length']<=150)]\n",
    "    data = data[(data['ROUGE'] >= 0.1) & (data['ROUGE'] <= 0.4)]\n",
    "    print(data.shape)\n",
    "    with open (save_path + data_type + '.source', 'w') as f:\n",
    "        for item in list(data['src_sent']):\n",
    "            f.write(item + '\\n')\n",
    "    with open (save_path + data_type + '.target', 'w') as f:\n",
    "        for item in list(data['tgt_sent']):\n",
    "            f.write(str(item) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45280, 6)\n",
      "(33919, 8)\n",
      "(11295, 6)\n",
      "(8485, 8)\n",
      "(6311, 6)\n",
      "(4753, 8)\n"
     ]
    }
   ],
   "source": [
    "### FixedROUGE, background summarization, before the 2nd pairs, input paired source, output paired target \n",
    "### train set source: 10.0 - 200.0\n",
    "### train set target: 10.0 - 150.0\n",
    "### train set ROUGE: 0.1 - 0.4\n",
    "save_path = './ordered_pairs_src_200_tgt_150_rouge_0.1-0.4_FixedROUGE_background_before_2ndPair/'\n",
    "for data_type in ['train', 'val', 'test']:\n",
    "    data = pd.read_csv('./src_100-1000_tgt_0-700_full_data_extract_elife_annals_medicine_reproductive_ordered_pairs_FixedROUGE/extract_ordered_pairs_background_before_2ndPair_FixedROUGE/'+data_type+'.csv')\n",
    "    print(data.shape)\n",
    "    data = data.dropna(subset=['src_sent'])\n",
    "    data = data.dropna(subset=['tgt_sent'])\n",
    "\n",
    "    sentences = list(data['src_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['src_length'] = length_list\n",
    "\n",
    "    sentences = list(data['tgt_sent'])\n",
    "    length_list = []\n",
    "    for s in sentences:\n",
    "        length_list.append(len(word_tokenize(s)))\n",
    "    data['tgt_length'] = length_list\n",
    "\n",
    "    data = data[(data['src_length']>=10) & (data['src_length']<=200)]\n",
    "    data = data[(data['tgt_length']>=10) & (data['tgt_length']<=150)]\n",
    "    data = data[(data['ROUGE'] >= 0.1) & (data['ROUGE'] <= 0.4)]\n",
    "    print(data.shape)\n",
    "    with open (save_path + data_type + '.source', 'w') as f:\n",
    "        for item in list(data['src_sent']):\n",
    "            f.write(item + '\\n')\n",
    "    with open (save_path + data_type + '.target', 'w') as f:\n",
    "        for item in list(data['tgt_sent']):\n",
    "            f.write(str(item) + '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('sum')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1924bd30786084701db1440cf02e7eded38cf7bfacfd8fb8021f34829d05af56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
